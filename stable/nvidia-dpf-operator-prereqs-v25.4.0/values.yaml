pack:
  namespace: ovn-kubernetes
charts:
  ## installed dependencies except for kamaji.
  argo-cd:
    enabled: true
    dex:
      enabled: false
    notifications:
      enabled: false
    global:
      podLabels:
        ovn.dpu.nvidia.com/skip-injection: ""
      affinity:
        nodeAffinity:
          # -- Default node affinity rules. Either: `none`, `soft` or `hard`
          type: soft
          # -- Default match expressions for node affinity
          matchExpressions:
            - key: "node-role.kubernetes.io/master"
              operator: Exists
            - key: "node-role.kubernetes.io/control-plane"
              operator: Exists
      tolerations:
        - key: node-role.kubernetes.io/master
          operator: Exists
          effect: NoSchedule
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule

  ## NFD configuration
  node-feature-discovery:
    enabled: true
    master:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: "node-role.kubernetes.io/master"
                    operator: Exists
              - matchExpressions:
                  - key: "node-role.kubernetes.io/control-plane"
                    operator: Exists
    worker:
      enable: true
      hostNetwork: true
      tolerations:
        - key: node.kubernetes.io/not-ready
          operator: Exists
      extraEnvs:
        - name: "KUBERNETES_SERVICE_HOST"
          value: "1.2.3.4"
        - name: "KUBERNETES_SERVICE_PORT"
          value: "6443"
      config:
        sources:
          pci:
            deviceClassWhitelist:
              - "0200"
            deviceLabelFields:
              - "class"
              - "vendor"
              - "device"
    gc:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: "node-role.kubernetes.io/master"
                    operator: Exists
              - matchExpressions:
                  - key: "node-role.kubernetes.io/control-plane"
                    operator: Exists
      tolerations:
        - key: node-role.kubernetes.io/master
          operator: Exists
          effect: NoSchedule
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule

  ## kamaji configuration.
  kamaji-etcd:
    enabled: true
    persistentVolumeClaim:
      storageClassName: spectro-storage-class
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: "node-role.kubernetes.io/master"
                  operator: Exists
            - matchExpressions:
                - key: "node-role.kubernetes.io/control-plane"
                  operator: Exists
    tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
    datastore:
      enabled: true
      annotations:
        helm.sh/resource-policy: keep
      name: default
  ## Kamaji etcd-defrag configuration.
  kamaji-etcd-defrag:
    enabled: true
    ## The schedule for the etcd-defrag job.
    schedule: 0 0 * * *
    image: ghcr.io/ahrtr/etcd-defrag:v0.22.0
    ## The defrag rule for the etcd-defrag job.
    ## See: https://github.com/ahrtr/etcd-defrag?tab=readme-ov-file#defragmentation-rule
    defragRule: "dbQuotaUsage > 0.8 || dbSize - dbSizeInUse > 200*1024*1024"
    ## # Keep only the X recent successful jobs.
    successfulJobsHistoryLimit: 3
    ## Limit the number of retries on failure
    backoffLimit: 6
  kamaji:
    enabled: true
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: "node-role.kubernetes.io/master"
                  operator: Exists
            - matchExpressions:
                - key: "node-role.kubernetes.io/control-plane"
                  operator: Exists
    tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
    etcd:
      deploy: false
    datastore:
      enabled: false
      nameOverride: default
    image:
      pullPolicy: IfNotPresent
    cfssl:
      image:
        tag: v1.6.5

  ## prometheus configuration.
  prometheus:
    enabled: false
  ## Unfortunately, there is no affinity and tolerations value in the prometheus helm chart.
  ## This is only necessary for dev purposes anyway. We should not run this stack on our control planes.
    alertmanager:
      enabled: false
    prometheus-node-exporter:
      enabled: false
    prometheus-pushgateway:
      enabled: false
    # We have to disable kube-state-metrics that is a dependency from Prometheus.
    # Adding KSM manually has the benefit that we can only enable KSM, without Prometheus.
    kube-state-metrics:
      enabled: false
    # Scrape metrics from deployed providers
    extraScrapeConfigs: |
      - job_name: 'doca-platform-framework'
        # 15s is a bit often for production but helps to get metrics quicker for development.
        scrape_interval: 15s
        metrics_path: /metrics
        scheme: https
        authorization:
          type: Bearer
          credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_dpu_nvidia_com_component]
            action: keep
            regex: ".*-controller-manager"
          - source_labels: [__meta_kubernetes_pod_container_port_name]
            action: keep
            regex: metrics

  ## grafana configuration
  grafana:
    enabled: false
    persistence:
      enabled: true
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
          - name: prometheus
            type: prometheus
            url: http://dpf-operator-prometheus-server
            access: proxy
            isDefault: true
    dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'default'
          orgId: 1
          folder: ''
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default
        - name: 'debug'
          orgId: 1
          folder: 'debug'
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/debug
    dashboardsConfigMaps:
      default: dpf-operator-grafana-dashboards
      debug: dpf-operator-grafana-debug-dashboards

  ## kube-state-metrics configuration
  kube-state-metrics:
    enabled: false
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: "node-role.kubernetes.io/master"
                  operator: Exists
            - matchExpressions:
                - key: "node-role.kubernetes.io/control-plane"
                  operator: Exists
    tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
    extraArgs:
      - --custom-resource-state-config-file=/etc/customresourcestate/config.yaml
    volumes:
      - configMap:
          defaultMode: 420
          name: dpf-operator-customresourcestate-config
        name: customresourcestate-config
    volumeMounts:
      - mountPath: /etc/customresourcestate
        name: customresourcestate-config
        readOnly: true
    rbac:
      extraRules:
        - apiGroups:
            - svc.dpu.nvidia.com
            - operator.dpu.nvidia.com
            - svc.dpu.nvidia.com
            - provisioning.dpu.nvidia.com
          resources:
            - dpuclusters
            - dpuclusters/status
            - dpudeployments
            - dpudeployments/status
            - dpuservicecredentialrequests
            - dpuservicecredentialrequests/status
            - dpuservices
            - dpuservices/status
            - dpuservicetemplates
            - dpuservicetemplates/status
            - dpfoperatorconfigs
            - dpfoperatorconfigs/status
            - dpuservicechains
            - dpuservicechains/status
            - dpuserviceinterfaces
            - dpuserviceinterfaces/status
            - dpuserviceipams
            - dpuserviceipams/status
            - dpus
            - dpus/status
            - bfbs
            - bfbs/status
          verbs: ["list", "watch"]
        - apiGroups: ["apiextensions.k8s.io"]
          resources: ["customresourcedefinitions"]
          verbs: ["list", "watch"]

  ## maintenance-operator chart configuration
  maintenance-operator-chart:
    fullnameOverride: maintenance-operator
    enabled: true
    operatorConfig:
      maxParallelOperations: 60%
    operator:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: "node-role.kubernetes.io/master"
                    operator: Exists
              - matchExpressions:
                  - key: "node-role.kubernetes.io/control-plane"
                    operator: Exists
      tolerations:
        - key: node-role.kubernetes.io/master
          operator: Exists
          effect: NoSchedule
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule

  parca:
    enabled: false
    agent:
      ## Disable the agent as it does not work in containerized environments.
      enabled: false
    serviceAccount:
      name: dpf-operator-parca
    server:
      tolerations:
        - key: node-role.kubernetes.io/master
          operator: Exists
          effect: NoSchedule
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule
      logLevel: info
      scrapeConfigs:
        - job_name: doca-platform
          kubernetes_sd_configs:
            - role: pod
          relabel_configs:
            ## Add each of the DPF controllers as a target for parca to scrape
            - source_labels: [__meta_kubernetes_pod_label_dpu_nvidia_com_component]
              action: keep
              regex: ".*-controller-manager"
            - source_labels: [__address__]
              regex: '([^:]+):.*'
              target_label: __address__
              ## The port value hardcoded for pprof here is the default for all controllers in DPF.
              replacement: '${1}:8082'
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: replace
              source_labels:
                - __meta_kubernetes_namespace
              target_label: kubernetes_namespace
            - action: replace
              source_labels:
                - __meta_kubernetes_pod_name
              target_label: kubernetes_pod_name
        - job_name: argo-cd
          kubernetes_sd_configs:
            - role: pod
          relabel_configs:
            ## Add argoCD as a target for parca to scrape
            - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_component]
              action: keep
              regex: "application-controller"
            - source_labels: [__address__]
              regex: '([^:]+):.*'
              target_label: __address__
              ## The port value hardcoded for pprof here is the default for argoCD controllers
              replacement: '${1}:8082'
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: replace
              source_labels:
                - __meta_kubernetes_namespace
              target_label: kubernetes_namespace
            - action: replace
              source_labels:
                - __meta_kubernetes_pod_name
              target_label: kubernetes_pod_name
  ## IsOpenShift templates resources - for example ClusterRoleBindings to SecurityContextConstraints - which are relevant
  ## when installing DPF using helm on OpenShift.
  isOpenshift: false