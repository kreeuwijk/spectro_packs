presets:
  - name: "single-node-cluster"
    displayName: "Single node cluster"
    group: "Cluster Configuration"
    remove: 
      - charts.rook-ceph-cluster.configOverride
    add: |
      charts:
        rook-ceph-cluster:
          configOverride: |
            [global]
            osd_pool_default_size = 1
            mon_warn_on_pool_no_redundancy = false
            bdev_flock_retry = 20
            bluefs_buffered_io = false
            mon_data_avail_warn = 10

          cephClusterSpec:
            mon:
              # Set the number of mons to be started. Generally recommended to be 3.
              # For highest availability, an odd number of mons should be specified.
              count: 1
              # The mons should be on unique nodes. For production, at least 3 nodes are recommended for this reason.
              # Mons should only be allowed on the same node for test environments where data loss is acceptable.
              allowMultiplePerNode: true

            mgr:
              # When higher availability of the mgr is needed, increase the count to 2.
              # In that case, one mgr will be active and one in standby. When Ceph updates which
              # mgr is active, Rook will update the mgr services to match the active mgr.
              count: 1
              allowMultiplePerNode: true

  - name: "multi-node-cluster"
    displayName: "Multi node cluster"
    group: "Cluster Configuration"
    remove: 
      - charts.rook-ceph-cluster.configOverride
    add: |
      charts:
        rook-ceph-cluster:
          configOverride: |
            [global]
            osd_max_backfills = 10

          cephClusterSpec:
            mon:
              # Set the number of mons to be started. Generally recommended to be 3.
              # For highest availability, an odd number of mons should be specified.
              count: 3
              # The mons should be on unique nodes. For production, at least 3 nodes are recommended for this reason.
              # Mons should only be allowed on the same node for test environments where data loss is acceptable.
              allowMultiplePerNode: false

            mgr:
              # When higher availability of the mgr is needed, increase the count to 2.
              # In that case, one mgr will be active and one in standby. When Ceph updates which
              # mgr is active, Rook will update the mgr services to match the active mgr.
              count: 2
              allowMultiplePerNode: false

  - name: "enable-block-storage"
    displayName: "Enable Block Storage Poool"
    group: "Storage Configuration - Block"
    remove: 
      - charts.rook-ceph-cluster.cephBlockPools
    add: |
      charts:
        rook-ceph-cluster:
          cephBlockPools:
            - name: ceph-blockpool
              # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Block-Storage/ceph-block-pool-crd.md#spec for available configuration
              spec:
                failureDomain: host
                replicated:
                  size: 3
                # Enables collecting RBD per-image IO statistics by enabling dynamic OSD performance counters. Defaults to false.
                # For reference: https://docs.ceph.com/docs/latest/mgr/prometheus/#rbd-io-statistics
                # enableRBDStats: true
              storageClass:
                enabled: true
                name: ceph-block
                annotations: {}
                labels: {}
                isDefault: false
                reclaimPolicy: Delete
                allowVolumeExpansion: true
                volumeBindingMode: "Immediate"
                mountOptions: []
                # see https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies
                allowedTopologies: []
                #        - matchLabelExpressions:
                #            - key: rook-ceph-role
                #              values:
                #                - storage-node
                # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Block-Storage-RBD/block-storage.md#provision-storage for available configuration
                parameters:
                  # (optional) mapOptions is a comma-separated list of map options.
                  # For krbd options refer
                  # https://docs.ceph.com/docs/latest/man/8/rbd/#kernel-rbd-krbd-options
                  # For nbd options refer
                  # https://docs.ceph.com/docs/latest/man/8/rbd-nbd/#options
                  # mapOptions: lock_on_read,queue_depth=1024

                  # (optional) unmapOptions is a comma-separated list of unmap options.
                  # For krbd options refer
                  # https://docs.ceph.com/docs/latest/man/8/rbd/#kernel-rbd-krbd-options
                  # For nbd options refer
                  # https://docs.ceph.com/docs/latest/man/8/rbd-nbd/#options
                  # unmapOptions: force

                  # RBD image format. Defaults to "2".
                  imageFormat: "2"

                  # RBD image features, equivalent to OR'd bitfield value: 63
                  # Available for imageFormat: "2". Older releases of CSI RBD
                  # support only the `layering` feature. The Linux kernel (KRBD) supports the
                  # full feature complement as of 5.4
                  imageFeatures: layering

                  # These secrets contain Ceph admin credentials.
                  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
                  csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
                  csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
                  csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
                  # Specify the filesystem type of the volume. If not specified, csi-provisioner
                  # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
                  # in hyperconverged settings where the volume is mounted on the same node as the osds.
                  csi.storage.k8s.io/fstype: ext4

  - name: "disable-block-storage"
    displayName: "Disable Block Storage Poool"
    group: "Storage Configuration - Block"
    remove: 
      - charts.rook-ceph-cluster.cephBlockPools
    add: |
      charts:
        rook-ceph-cluster:
          cephBlockPools: []

  - name: "enable-file-storage"
    displayName: "Enable File Storage Poool"
    group: "Storage Configuration - File"
    remove: 
      - charts.rook-ceph-cluster.cephFileSystems
    add: |
      charts:
        rook-ceph-cluster:
          cephFileSystems:
            - name: ceph-filesystem
              # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
              spec:
                metadataPool:
                  replicated:
                    size: 3
                dataPools:
                  - failureDomain: host
                    replicated:
                      size: 3
                    # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
                    name: data0
                metadataServer:
                  activeCount: 1
                  activeStandby: true
                  resources:
                    limits:
                      memory: "4Gi"
                    requests:
                      cpu: "1000m"
                      memory: "4Gi"
                  priorityClassName: system-cluster-critical
              storageClass:
                enabled: true
                isDefault: true
                annotations: {}
                labels: {}
                name: ceph-filesystem
                # (Optional) specify a data pool to use, must be the name of one of the data pools above, 'data0' by default
                pool: data0
                reclaimPolicy: Delete
                allowVolumeExpansion: true
                volumeBindingMode: "Immediate"
                annotations: {}
                labels: {}
                mountOptions: []
                # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage.md#provision-storage for available configuration
                parameters:
                  # The secrets contain Ceph admin credentials.
                  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
                  csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
                  csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
                  csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
                  # Specify the filesystem type of the volume. If not specified, csi-provisioner
                  # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
                  # in hyperconverged settings where the volume is mounted on the same node as the osds.
                  csi.storage.k8s.io/fstype: ext4

  - name: "disable-file-storage"
    displayName: "Enable File Storage Poool"
    group: "Storage Configuration - File"
    remove: 
      - charts.rook-ceph-cluster.cephFileSystems
    add: |
      charts:
        rook-ceph-cluster:
          cephFileSystems: []

  - name: "create-pvc-based-osds"
    displayName: "Create PVC-based OSDs"
    group: "vSphere storageClassDeviceSets"
    remove: []
    add: |
      charts:
        rook-ceph-cluster:
          cephClusterSpec:
            mon:
              volumeClaimTemplate:
                spec:
                  storageClassName: spectro-storage-class
                  resources:
                    requests:
                      storage: 10Gi
            storage:
              onlyApplyOSDPlacement: false
              storageClassDeviceSets:
                - name: set1
                  # The number of OSDs to create from this device set
                  count: 3
                  portable: true
                  tuneDeviceClass: false
                  tuneFastDeviceClass: true
                  encrypted: false

                  placement: 
                    topologySpreadConstraints:
                      - maxSkew: 1
                        topologyKey: kubernetes.io/hostname
                        whenUnsatisfiable: ScheduleAnyway
                        labelSelector:
                          matchExpressions:
                            - key: app
                              operator: In
                              values:
                                - rook-ceph-osd
                  preparePlacement:
                    podAntiAffinity:
                      preferredDuringSchedulingIgnoredDuringExecution:
                        - weight: 100
                          podAffinityTerm:
                            labelSelector:
                              matchExpressions:
                                - key: app
                                  operator: In 
                                  values:
                                    - rook-ceph-osd
                                - key: app
                                  operator: In
                                  values:
                                    - rook-ceph-osd-prepare
                            topologyKey: kubernetes.io/hostname
                    topologySpreadConstraints:
                      - maxSkew: 1
                        # IMPORTANT: If you don't have zone labels, change this to another key such as kubernetes.io/hostname
                        topologyKey: topology.kubernetes.io/zone
                        whenUnsatisfiable: DoNotSchedule
                        labelSelector:
                          matchExpressions:
                            - key: app
                              operator: In
                              values:
                                - rook-ceph-osd-prepare

                  volumeClaimTemplates:
                    - metadata:
                        name: data
                        # if you are looking at giving your OSD a different CRUSH device class than the one detected by Ceph
                        # annotations:
                        #   crushDeviceClass: hybrid
                      spec:
                        resources:
                          requests:
                            storage: 100Gi
                        # Using vsphere-csi storage-class to provision disks
                        storageClassName: spectro-storage-class
                        volumeMode: Block
                        accessModes:
                          - ReadWriteOnce

  - name: "dont-create-pvc-based-osds"
    displayName: "Do not create PVC-based OSDs"
    group: "vSphere storageClassDeviceSets"
    remove:
      - charts.rook-ceph-cluster.cephClusterSpec.mon.volumeClaimTemplate
      - charts.rook-ceph-cluster.cephClusterSpec.storage.onlyApplyOSDPlacement
      - charts.rook-ceph-cluster.cephClusterSpec.storage.storageClassDeviceSets

  - name: "1-pvc-based-osd"
    displayName: "1 PVC-based OSD (single-node)"
    group: "vSphere storageClassDeviceSets amount"
    remove: []
    add: |
      charts:
        rook-ceph-cluster:
          cephClusterSpec:
            storage:
              storageClassDeviceSets:
                - name: set1
                  count: 1

  - name: "3-pvc-based-osd"
    displayName: "3 PVC-based OSDs (multi-node)"
    group: "vSphere storageClassDeviceSets amount"
    remove: []
    add: |
      charts:
        rook-ceph-cluster:
          cephClusterSpec:
            storage:
              storageClassDeviceSets:
                - name: set1
                  count: 3