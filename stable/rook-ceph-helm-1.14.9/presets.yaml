presets:
  - name: "single-node-cluster"
    displayName: "Single node cluster"
    group: "Cluster Configuration"
    remove: 
      - charts.rook-ceph-cluster.configOverride
    add: |
      charts:
        rook-ceph-cluster:
          configOverride: |
            [global]
            osd_pool_default_size = 1
            mon_warn_on_pool_no_redundancy = false
            bdev_flock_retry = 20
            bluefs_buffered_io = false
            mon_data_avail_warn = 10

          cephClusterSpec:
            mon:
              # Set the number of mons to be started. Generally recommended to be 3.
              # For highest availability, an odd number of mons should be specified.
              count: 1
              # The mons should be on unique nodes. For production, at least 3 nodes are recommended for this reason.
              # Mons should only be allowed on the same node for test environments where data loss is acceptable.
              allowMultiplePerNode: true

            mgr:
              # When higher availability of the mgr is needed, increase the count to 2.
              # In that case, one mgr will be active and one in standby. When Ceph updates which
              # mgr is active, Rook will update the mgr services to match the active mgr.
              count: 1
              allowMultiplePerNode: true

  - name: "multi-node-cluster"
    displayName: "Multi node cluster"
    group: "Cluster Configuration"
    remove: 
      - charts.rook-ceph-cluster.configOverride
    add: |
      charts:
        rook-ceph-cluster:
          configOverride: |
            [global]
            osd_max_backfills = 10

          cephClusterSpec:
            mon:
              # Set the number of mons to be started. Generally recommended to be 3.
              # For highest availability, an odd number of mons should be specified.
              count: 3
              # The mons should be on unique nodes. For production, at least 3 nodes are recommended for this reason.
              # Mons should only be allowed on the same node for test environments where data loss is acceptable.
              allowMultiplePerNode: false

            mgr:
              # When higher availability of the mgr is needed, increase the count to 2.
              # In that case, one mgr will be active and one in standby. When Ceph updates which
              # mgr is active, Rook will update the mgr services to match the active mgr.
              count: 2
              allowMultiplePerNode: false

  - name: "enable-block-storage"
    displayName: "Enable Block Storage Poool"
    group: "Storage Configuration - Block"
    remove: 
      - charts.rook-ceph-cluster.cephBlockPools
    add: |
      charts:
        rook-ceph-cluster:
          cephBlockPools:
            - name: ceph-blockpool
              # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Block-Storage/ceph-block-pool-crd.md#spec for available configuration
              spec:
                failureDomain: host
                replicated:
                  size: 3
                # Enables collecting RBD per-image IO statistics by enabling dynamic OSD performance counters. Defaults to false.
                # For reference: https://docs.ceph.com/docs/latest/mgr/prometheus/#rbd-io-statistics
                # enableRBDStats: true
              storageClass:
                enabled: true
                name: ceph-block
                annotations: {}
                labels: {}
                isDefault: false
                reclaimPolicy: Delete
                allowVolumeExpansion: true
                volumeBindingMode: "Immediate"
                mountOptions: []
                # see https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies
                allowedTopologies: []
                #        - matchLabelExpressions:
                #            - key: rook-ceph-role
                #              values:
                #                - storage-node
                # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Block-Storage-RBD/block-storage.md#provision-storage for available configuration
                parameters:
                  # (optional) mapOptions is a comma-separated list of map options.
                  # For krbd options refer
                  # https://docs.ceph.com/docs/latest/man/8/rbd/#kernel-rbd-krbd-options
                  # For nbd options refer
                  # https://docs.ceph.com/docs/latest/man/8/rbd-nbd/#options
                  # mapOptions: lock_on_read,queue_depth=1024

                  # (optional) unmapOptions is a comma-separated list of unmap options.
                  # For krbd options refer
                  # https://docs.ceph.com/docs/latest/man/8/rbd/#kernel-rbd-krbd-options
                  # For nbd options refer
                  # https://docs.ceph.com/docs/latest/man/8/rbd-nbd/#options
                  # unmapOptions: force

                  # RBD image format. Defaults to "2".
                  imageFormat: "2"

                  # RBD image features, equivalent to OR'd bitfield value: 63
                  # Available for imageFormat: "2". Older releases of CSI RBD
                  # support only the `layering` feature. The Linux kernel (KRBD) supports the
                  # full feature complement as of 5.4
                  imageFeatures: layering

                  # These secrets contain Ceph admin credentials.
                  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
                  csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
                  csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
                  csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
                  # Specify the filesystem type of the volume. If not specified, csi-provisioner
                  # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
                  # in hyperconverged settings where the volume is mounted on the same node as the osds.
                  csi.storage.k8s.io/fstype: ext4

  - name: "disable-block-storage"
    displayName: "Disable Block Storage Poool"
    group: "Storage Configuration - Block"
    remove: 
      - charts.rook-ceph-cluster.cephBlockPools

  - name: "enable-file-storage"
    displayName: "Enable File Storage Poool"
    group: "Storage Configuration - File"
    remove: 
      - charts.rook-ceph-cluster.cephFileSystems
    add: |
      charts:
        rook-ceph-cluster:
          cephFileSystems:
            - name: ceph-filesystem
              # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
              spec:
                metadataPool:
                  replicated:
                    size: 3
                dataPools:
                  - failureDomain: host
                    replicated:
                      size: 3
                    # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
                    name: data0
                metadataServer:
                  activeCount: 1
                  activeStandby: true
                  resources:
                    limits:
                      memory: "4Gi"
                    requests:
                      cpu: "1000m"
                      memory: "4Gi"
                  priorityClassName: system-cluster-critical
              storageClass:
                enabled: true
                isDefault: true
                name: ceph-filesystem
                # (Optional) specify a data pool to use, must be the name of one of the data pools above, 'data0' by default
                pool: data0
                reclaimPolicy: Delete
                allowVolumeExpansion: true
                volumeBindingMode: "Immediate"
                annotations: {}
                labels: {}
                mountOptions: []
                # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage.md#provision-storage for available configuration
                parameters:
                  # The secrets contain Ceph admin credentials.
                  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
                  csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
                  csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
                  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
                  csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
                  # Specify the filesystem type of the volume. If not specified, csi-provisioner
                  # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
                  # in hyperconverged settings where the volume is mounted on the same node as the osds.
                  csi.storage.k8s.io/fstype: ext4

  - name: "disable-file-storage"
    displayName: "Enable File Storage Poool"
    group: "Storage Configuration - File"
    remove: 
      - charts.rook-ceph-cluster.cephFileSystems