# Default values for elastic-fluentd-kibana.

### Elastic search chart values ###
elasticsearch:

  #Note: If you change the default fullname below, you may want to update fluentd and kibana charts elasticsearch host references
  fullnameOverride: elasticsearch
  appVersion: "6.7.0"

  ## Define serviceAccount names for components. Defaults to component's fully qualified name.
  ##
  serviceAccounts:
    client:
      create: true
      name:
    master:
      create: true
      name:
    data:
      create: true
      name:

  ## Specify if a Pod Security Policy for node-exporter must be created
  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  ##
  podSecurityPolicy:
    enabled: false
    annotations: {}
      ## Specify pod annotations
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
      ##
      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
    # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
    # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

  securityContext:
    enabled: false
    runAsUser: 1000

  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName: "default-scheduler"

  image:
    repository: "docker.elastic.co/elasticsearch/elasticsearch-oss"
    tag: "6.7.0"
    pullPolicy: "IfNotPresent"
    # If specified, use these secrets to access the image
    # pullSecrets:
    #   - registry-secret

  testFramework:
    image: "dduportal/bats"
    tag: "0.4.0"

  initImage:
    repository: "busybox"
    tag: "latest"
    pullPolicy: "Always"

  cluster:
    name: "elasticsearch"
    # If you want X-Pack installed, switch to an image that includes it, enable this option and toggle the features you want
    # enabled in the environment variables outlined in the README
    xpackEnable: false
    # Some settings must be placed in a keystore, so they need to be mounted in from a secret.
    # Use this setting to specify the name of the secret
    # keystoreSecret: eskeystore
    config: {}
    # Custom parameters, as string, to be added to ES_JAVA_OPTS environment variable
    additionalJavaOpts: ""
    # Command to run at the end of deployment
    bootstrapShellCommand: ""
    env:
      # IMPORTANT: https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html#minimum_master_nodes
      # To prevent data loss, it is vital to configure the discovery.zen.minimum_master_nodes setting so that each master-eligible
      # node knows the minimum number of master-eligible nodes that must be visible in order to form a cluster.
      MINIMUM_MASTER_NODES: "2"
    # List of plugins to install via dedicated init container
    plugins: []
    # - ingest-attachment
    # - mapper-size

    loggingYml:
      # you can override this using by setting a system property, for example -Des.logger.level=DEBUG
      es.logger.level: INFO
      rootLogger: ${es.logger.level}, console
      logger:
        # log action execution errors for easier debugging
        action: DEBUG
        # reduce the logging for aws, too much is logged under the default INFO
        com.amazonaws: WARN
      appender:
        console:
          type: console
          layout:
            type: consolePattern
            conversionPattern: "[%d{ISO8601}][%-5p][%-25c] %m%n"

    log4j2Properties: |
      status = error
      appender.console.type = Console
      appender.console.name = console
      appender.console.layout.type = PatternLayout
      appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] %marker%m%n
      rootLogger.level = info
      rootLogger.appenderRef.console.ref = console
      logger.searchguard.name = com.floragunn
      logger.searchguard.level = info

  client:
    name: client
    replicas: 2
    serviceType: ClusterIP
    ## If coupled with serviceType = "NodePort", this will set a specific nodePort to the client HTTP port
    # httpNodePort: 30920
    loadBalancerIP: {}
    loadBalancerSourceRanges: {}
    ## (dict) If specified, apply these annotations to the client service
    #  serviceAnnotations:
    #    example: client-svc-foo
    heapSize: "512m"
    # additionalJavaOpts: "-XX:MaxRAM=512m"
    antiAffinity: "soft"
    nodeAffinity: {}
    nodeSelector: {}
    tolerations: []
    # terminationGracePeriodSeconds: 60
    initResources: {}
      # limits:
      #   cpu: "25m"
      #   # memory: "128Mi"
      # requests:
    #   cpu: "25m"
    #   memory: "128Mi"
    resources:
      limits:
        cpu: "500m"
        # memory: "1024Mi"
      requests:
        cpu: "25m"
        memory: "512Mi"
    priorityClassName: ""
    ## (dict) If specified, apply these annotations to each client Pod
    # podAnnotations:
    #   example: client-foo
    podDisruptionBudget:
      enabled: false
      minAvailable: 1
      # maxUnavailable: 1
    hooks: {}
      ## (string) Script to execute prior the client pod stops.
      # preStop: |-

    ## (string) Script to execute after the client pod starts.
    # postStart: |-
    ingress:
      enabled: false
      # user: NAME
      # password: PASSWORD
      annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      path: /
      hosts:
        - chart-example.local
      tls: []
      #  - secretName: chart-example-tls
      #    hosts:
      #      - chart-example.local

  master:
    name: master
    exposeHttp: false
    replicas: 3
    heapSize: "512m"
    # additionalJavaOpts: "-XX:MaxRAM=512m"
    persistence:
      enabled: false
      accessMode: ReadWriteOnce
      name: data
      size: "5Gi"
    readinessProbe:
      httpGet:
        path: /_cluster/health?local=true
        port: 9200
      initialDelaySeconds: 5
    antiAffinity: "soft"
    nodeAffinity: {}
    nodeSelector: {}
    tolerations: []
    # terminationGracePeriodSeconds: 60
    initResources: {}
      # limits:
      #   cpu: "25m"
      #   # memory: "128Mi"
      # requests:
    #   cpu: "25m"
    #   memory: "128Mi"
    resources:
      limits:
        cpu: "300m"
        # memory: "1024Mi"
      requests:
        cpu: "25m"
        memory: "512Mi"
    priorityClassName: ""
    ## (dict) If specified, apply these annotations to each master Pod
    # podAnnotations:
    #   example: master-foo
    podManagementPolicy: OrderedReady
    podDisruptionBudget:
      enabled: false
      minAvailable: 2  # Same as `cluster.env.MINIMUM_MASTER_NODES`
      # maxUnavailable: 1
    updateStrategy:
      type: OnDelete
    hooks: {}
      ## (string) Script to execute prior the master pod stops.
      # preStop: |-

    ## (string) Script to execute after the master pod starts.
    # postStart: |-

  data:
    name: data
    exposeHttp: false
    replicas: 2
    heapSize: "1536m"
    # additionalJavaOpts: "-XX:MaxRAM=1536m"
    persistence:
      enabled: true
      accessMode: ReadWriteOnce
      name: data
      size: "40Gi"
    readinessProbe:
      httpGet:
        path: /_cluster/health?local=true
        port: 9200
      initialDelaySeconds: 5
    terminationGracePeriodSeconds: 3600
    antiAffinity: "soft"
    nodeAffinity: {}
    nodeSelector: {}
    tolerations: []
    initResources: {}
      # limits:
      #   cpu: "25m"
      #   # memory: "128Mi"
      # requests:
    #   cpu: "25m"
    #   memory: "128Mi"
    resources:
      limits:
        cpu: "500m"
        memory: "4096Mi"
      requests:
        cpu: "25m"
        memory: "2048Mi"
    priorityClassName: ""
    ## (dict) If specified, apply these annotations to each data Pod
    # podAnnotations:
    #   example: data-foo
    podDisruptionBudget:
      enabled: false
      # minAvailable: 1
      maxUnavailable: 1
    podManagementPolicy: OrderedReady
    updateStrategy:
      type: OnDelete
    hooks:
      ## Drain the node before stopping it and re-integrate it into the cluster after start.
      ## When enabled, it supersedes `data.hooks.preStop` and `data.hooks.postStart` defined below.
      drain:
        enabled: true

      ## (string) Script to execute prior the data pod stops. Ignored if `data.hooks.drain.enabled` is true (default)
      # preStop: |-
      #   #!/bin/bash
      #   exec &> >(tee -a "/var/log/elasticsearch-hooks.log")
      #   NODE_NAME=${HOSTNAME}
      #   curl -s -XPUT -H 'Content-Type: application/json' '{{ template "elasticsearch.client.fullname" . }}:9200/_cluster/settings' -d "{
      #     \"transient\" :{
      #         \"cluster.routing.allocation.exclude._name\" : \"${NODE_NAME}\"
      #     }
      #   }"
      #   echo "Node ${NODE_NAME} is exluded from the allocation"

      ## (string) Script to execute after the data pod starts. Ignored if `data.hooks.drain.enabled` is true (default)
      # postStart: |-
      #   #!/bin/bash
      #   exec &> >(tee -a "/var/log/elasticsearch-hooks.log")
      #   NODE_NAME=${HOSTNAME}
      #   CLUSTER_SETTINGS=$(curl -s -XGET "http://{{ template "elasticsearch.client.fullname" . }}:9200/_cluster/settings")
      #   if echo "${CLUSTER_SETTINGS}" | grep -E "${NODE_NAME}"; then
      #     echo "Activate node ${NODE_NAME}"
      #     curl -s -XPUT -H 'Content-Type: application/json' "http://{{ template "elasticsearch.client.fullname" . }}:9200/_cluster/settings" -d "{
      #       \"transient\" :{
      #           \"cluster.routing.allocation.exclude._name\" : null
      #       }
      #     }"
      #   fi
      #   echo "Node ${NODE_NAME} is ready to be used"

  ## Sysctl init container to setup vm.max_map_count
  # see https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html
  # and https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html#mlockall
  sysctlInitContainer:
    enabled: true
  ## Chown init container to change ownership of data and logs directories to elasticsearch user
  chownInitContainer:
    enabled: true
  ## Additional init containers
  extraInitContainers: |


#### Fluentd chart values ####
fluentd:
  fullnameOverride: fluentd
  image:
    repository: gcr.io/google-containers/fluentd-elasticsearch
    ## Specify an imagePullPolicy (Required)
    ## It's recommended to change this to 'Always' if the image tag is 'latest'
    ## ref: http://kubernetes.io/docs/user-guide/images/#updating-images
    tag: v2.4.0
    pullPolicy: IfNotPresent

  ## Configure resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    limits:
      cpu: 400m
      memory: 800Mi
    requests:
      cpu: 100m
      memory: 200Mi

  elasticsearch:
    host: elasticsearch-client
    port: 9200
    scheme: http
    ssl_version: TLSv1_2
    #Buffer plugin type used is file and the default buffer chunk limit for file type is 256M
    #https://docs.fluentd.org/configuration/buffer-section
    buffer_chunk_limit: 256M

  # If you want to add custom environment variables, use the env dict
  # You can then reference these in your config file e.g.:
  #     user "#{ENV['OUTPUT_USER']}"
  env:
  # OUTPUT_USER: my_user

  # If you want to add custom environment variables from secrets, use the secret list
  secret:
  # - name: ELASTICSEARCH_PASSWORD
  #   secret_name: elasticsearch
  #   secret_key: password

  rbac:
    create: true

  serviceAccount:
    # Specifies whether a ServiceAccount should be created
    create: true
    # The name of the ServiceAccount to use.
    # If not set and create is true, a name is generated using the fullname template
    name:

  ## Specify if a Pod Security Policy for node-exporter must be created
  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  ##
  podSecurityPolicy:
    enabled: false
    annotations: {}
      ## Specify pod annotations
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
      ##
      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
    # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
    # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

  livenessProbe:
    enabled: true

  annotations: {}

  podAnnotations: {}
  # prometheus.io/scrape: "true"
  # prometheus.io/port: "24231"

  ## DaemonSet update strategy
  ## Ref: https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/
  updateStrategy:
    type: RollingUpdate

  nodeSelector: {}

  # Fluentd is configured to run on all master and worker nodes
  tolerations:
  - key: node-role.kubernetes.io/master
    operator: Exists
    effect: NoSchedule
  - operator: "Exists"
    effect: "NoExecute"
  - operator: "Exists"
    effect: "NoSchedule"

  service: {}
    #type: ClusterIP
    #ports:
    #  - name: "monitor-agent"
    #    protocol: TCP
    #    containerPort: 24220

  configMaps:
    general.conf: |-
      # Prevent fluentd from handling records containing its own logs. Otherwise
      # it can lead to an infinite loop, when error in sending one message generates
      # another message which also fails to be sent and so on.
      <match fluentd.**>
        @type null
      </match>
      # Used for health checking
      <source>
        @type http
        port 9880
        bind 0.0.0.0
      </source>
      # Emits internal metrics to every minute, and also exposes them on port
      # 24220. Useful for determining if an output plugin is retryring/erroring,
      # or determining the buffer queue length.
      <source>
        @type monitor_agent
        bind 0.0.0.0
        port 24220
        tag fluentd.monitor.metrics
      </source>
    system.conf: |-
      <system>
        root_dir /tmp/fluentd-buffers/
      </system>
    forward-input.conf: |-
      <source>
        @type forward
        port 24224
        bind 0.0.0.0
      </source>
    containers.input.conf: |-
      # Logs from systemd-journal for Kubelet service
      <source>
        @type systemd
        @id in_systemd_kubelet
        path /var/log/journal
        matches [{ "_SYSTEMD_UNIT": "kubelet.service" }]
        <storage>
          @type local
          persistent true
          path /var/log/fluentd-journald-kubelet-cursor.json
        </storage>
        <entry>
          fields_strip_underscores true
        </entry>
        read_from_head true
        tag kubelet
      </source>
      # Logs from all the containers
      <source>
        @id fluentd-containers.log
        @type tail
        path /var/log/containers/*.log
        pos_file /var/log/es-containers.log.pos
        tag kubernetes.*
        read_from_head true
        <parse>
          @type multi_format
          #Docker logging format
          <pattern>
            format json
            time_key time
            time_format %Y-%m-%dT%H:%M:%S.%NZ
          </pattern>
          #Containerd logging format
          <pattern>
            format /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
            time_format %Y-%m-%dT%H:%M:%S.%N%:z
          </pattern>
          #Catch all for those logs which doesn't match log pattern
          <pattern>
            format none
          </pattern>
        </parse>
      </source>
      # Detect exceptions in the log output and forward them as one log entry.
      <match raw.kubernetes.**>
        @id raw.kubernetes
        @type detect_exceptions
        remove_tag_prefix raw
        message log
        stream stream
        multiline_flush_interval 5
        max_bytes 500000
        max_lines 1000
      </match>
      # Concatenate multi-line logs
      <filter **>
        @id filter_concat
        @type concat
        key message
        multiline_end_regexp /\n$/
        separator ""
      </filter>
      # Enriches records with Kubernetes metadata
      <filter kubernetes.**>
        @id filter_kubernetes_metadata
        @type kubernetes_metadata
      </filter>
      # Fixes json fields in Elasticsearch
      <filter kubernetes.**>
        @id filter_parser
        @type parser
        key_name log
        reserve_data true
        remove_key_name_field true
        <parse>
          @type multi_format
          <pattern>
            format json
          </pattern>
          <pattern>
            format none
          </pattern>
        </parse>
      </filter>
    output.conf: |-
      <match **>
        @id elasticsearch
        @type elasticsearch
        @log_level info
        include_tag_key true
        host "#{ENV['OUTPUT_HOST']}"
        port "#{ENV['OUTPUT_PORT']}"
        scheme "#{ENV['OUTPUT_SCHEME']}"
        ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"
        logstash_format true
        reload_connections false
        reconnect_on_error true
        reload_on_failure true
        request_timeout 60s
        <buffer>
          @type file
          path /var/log/fluentd-buffers/kubernetes.system.buffer
          flush_mode interval
          flush_interval 5s
          flush_thread_count 8
          retry_type exponential_backoff
          retry_forever false
          retry_timeout 72h
          retry_max_interval 10s
          retry_randomize true
          retry_wait 5s
          chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
          overflow_action block
        </buffer>
      </match>
  # extraVolumes:
  #   - name: es-certs
  #     secret:
  #       defaultMode: 420
  #       secretName: es-certs
  # extraVolumeMounts:
  #   - name: es-certs
  #     mountPath: /certs
  #     readOnly: true

#### Kibana charts values ####
kibana:
  fullnameOverride: kibana
  image:
    repository: "docker.elastic.co/kibana/kibana-oss"
    tag: "6.7.0"
    pullPolicy: "IfNotPresent"

  testFramework:
    enabled: "true"
    image: "dduportal/bats"
    tag: "0.4.0"

  commandline:
    args: []

  env:
    ## All Kibana configuration options are adjustable via env vars.
    ## To adjust a config option to an env var uppercase + replace `.` with `_`
    ## Ref: https://www.elastic.co/guide/en/kibana/current/settings.html
    ## For kibana < 6.6, use ELASTICSEARCH_URL instead
    # ELASTICSEARCH_HOSTS: http://elasticsearch-client:9200
    # SERVER_PORT: 5601
    ELASTICSEARCH_HOSTS: http://elasticsearch-client:9200
  # LOGGING_VERBOSE: "true"
  # SERVER_DEFAULTROUTE: "/app/kibana"

  envFromSecrets: {}
    ## Create a secret manually. Reference it here to inject environment variables
    # ELASTICSEARCH_USERNAME:
    #   from:
    #     secret: secret-name-here
    #     key: ELASTICSEARCH_USERNAME
    # ELASTICSEARCH_PASSWORD:
  #   from:
  #     secret: secret-name-here
  #     key: ELASTICSEARCH_PASSWORD

  files:
    kibana.yml:
      ## Default Kibana configuration from kibana-docker.
      server.name: kibana
      server.host: "0"
      ## For kibana < 6.6, use elasticsearch.url instead
      elasticsearch.hosts: http://elasticsearch-client:9200

      ## Custom config properties below
      ## Ref: https://www.elastic.co/guide/en/kibana/current/settings.html
      # server.port: 5601
      # logging.verbose: "true"
      # server.defaultRoute: "/app/kibana"

  deployment:
    annotations: {}

  service:
    type: LoadBalancer
    # clusterIP: None
    # portName: kibana-svc
    externalPort: 80
    internalPort: 5601

    # authProxyPort: 5602 To be used with authProxyEnabled and a proxy extraContainer
    ## External IP addresses of service
    ## Default: nil
    ##
    # externalIPs:
    # - 192.168.0.1
    #
    ## LoadBalancer IP if service.type is LoadBalancer
    ## Default: nil
    ##
    # loadBalancerIP: 10.2.2.2
    annotations: {}
    # Annotation example: setup ssl with aws cert when service.type is LoadBalancer
    # service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-1:EXAMPLE_CERT
    labels: {}
    ## Label example: show service URL in `kubectl cluster-info`
    # kubernetes.io/cluster-service: "true"
    ## Limit load balancer source ips to list of CIDRs (where available)
    # loadBalancerSourceRanges: []
    selector: {}

  ingress:
    enabled: false
      # hosts:
      # - kibana.localhost.localdomain
      # - localhost.localdomain/kibana
      # annotations:
      #   kubernetes.io/ingress.class: nginx
      #   kubernetes.io/tls-acme: "true"
      # tls:
    # - secretName: chart-example-tls
    #   hosts:
    #     - chart-example.local

  serviceAccount:
    # Specifies whether a service account should be created
    create: false
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    # If set and create is false, the service account must be existing
    name:

  livenessProbe:
    enabled: false
    path: /status
    initialDelaySeconds: 30
    timeoutSeconds: 10

  readinessProbe:
    enabled: false
    path: /status
    initialDelaySeconds: 30
    timeoutSeconds: 10
    periodSeconds: 10
    successThreshold: 5

  # Enable an authproxy. Specify container in extraContainers
  authProxyEnabled: false

  extraContainers: |
  # - name: proxy
  #   image: quay.io/gambol99/keycloak-proxy:latest
  #   args:
  #     - --resource=uri=/*
  #     - --discovery-url=https://discovery-url
  #     - --client-id=client
  #     - --client-secret=secret
  #     - --listen=0.0.0.0:5602
  #     - --upstream-url=http://127.0.0.1:5601
  #   ports:
  #     - name: web
  #       containerPort: 9090

  extraVolumeMounts: []

  extraVolumes: []

  resources:
    limits:
      cpu: 500m
      memory: 1024Mi
    requests:
      cpu: 100m
      memory: 512Mi

  priorityClassName: ""

  # Affinity for pod assignment
  # Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  # affinity: {}

  # Tolerations for pod assignment
  # Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  # Node labels for pod assignment
  # Ref: https://kubernetes.io/docs/user-guide/node-selection/
  nodeSelector: {}

  podAnnotations: {}
  replicaCount: 1
  revisionHistoryLimit: 3

  # Custom labels for pod assignment
  podLabels: {}

  # To export a dashboard from a running Kibana 6.3.x use:
  # curl --user <username>:<password> -XGET https://kibana.yourdomain.com:5601/api/kibana/dashboards/export?dashboard=<some-dashboard-uuid> > my-dashboard.json
  # A dashboard is defined by a name and a string with the json payload or the download url
  dashboardImport:
    enabled: true
    timeout: 900
    xpackauth:
      enabled: false
      username: myuser
      password: mypass
    dashboards:
      k8s: https://raw.githubusercontent.com/monotek/kibana-dashboards/master/k8s-fluentd-elasticsearch.json

  # List of plugins to install using initContainer
  # NOTE : We notice that lower resource constraints given to the chart + plugins are likely not going to work well.
  plugins:
    # set to true to enable plugins installation
    enabled: false
    # set to true to remove all kibana plugins before installation
    reset: false
    # Use <plugin_name,version,url> to add/upgrade plugin
    values:
    # - elastalert-kibana-plugin,1.0.1,https://github.com/bitsensor/elastalert-kibana-plugin/releases/download/1.0.1/elastalert-kibana-plugin-1.0.1-6.4.2.zip
    # - logtrail,0.1.31,https://github.com/sivasamyk/logtrail/releases/download/v0.1.31/logtrail-6.6.0-0.1.31.zip
    # - other_plugin

  persistentVolumeClaim:
    # set to true to use pvc
    enabled: false
    # set to true to use you own pvc
    existingClaim: false
    annotations: {}

    accessModes:
      - ReadWriteOnce
    size: "5Gi"
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

  # default security context
  securityContext:
    enabled: false
    allowPrivilegeEscalation: false
    runAsUser: 1000
    fsGroup: 2000

  extraConfigMapMounts: []
    # - name: logtrail-configs
  #   configMap: kibana-logtrail
  #   mountPath: /usr/share/kibana/plugins/logtrail/logtrail.json
  #   subPath: logtrail.json

  # Add your own init container or uncomment and modify the given example.
  initContainers: {}
    ## Don't start kibana till Elasticsearch is reachable.
    ## Ensure that it is available at http://elasticsearch:9200
    ##
    # es-check:  # <- will be used as container name
    #   image: "appropriate/curl:latest"
    #   imagePullPolicy: "IfNotPresent"
    #   command:
    #     - "/bin/sh"
    #     - "-c"
    #     - |
    #       is_down=true
    #       while "$is_down"; do
    #         if curl -sSf --fail-early --connect-timeout 5 http://elasticsearch:9200; then
    #           is_down=false
    #         else
  #           sleep 5
  #         fi
  #       done

### Elastic search curator chart values ###
elasticsearch-curator:
  fullnameOverride: elasticsearch-curator
  cronjob:
    # At 01:00 every day
    schedule: "0 1 * * *"
    annotations: {}
    labels: {}
    concurrencyPolicy: ""
    failedJobsHistoryLimit: ""
    successfulJobsHistoryLimit: ""
    jobRestartPolicy: Never

  pod:
    annotations: {}
    labels: {}

  rbac:
    # Specifies whether RBAC should be enabled
    enabled: false

  serviceAccount:
    # Specifies whether a ServiceAccount should be created
    create: true
    # The name of the ServiceAccount to use.
    # If not set and create is true, a name is generated using the fullname template
    name:

  psp:
    # Specifies whether a podsecuritypolicy should be created
    create: false

  image:
    repository: untergeek/curator
    tag: 5.7.6
    pullPolicy: IfNotPresent

  hooks:
    install: false
    upgrade: false

  # run curator in dry-run mode
  dryrun: false

  command: ["/curator/curator"]
  env: {}

  configMaps:
    # Delete indices older than 7 days
    action_file_yml: |-
      ---
      actions:
        1:
          action: delete_indices
          description: "Clean up ES by deleting old indices"
          options:
            timeout_override:
            continue_if_exception: False
            disable_action: False
            ignore_empty_list: True
          filters:
          - filtertype: pattern
            kind: prefix
            value: logstash-
          - filtertype: age
            source: name
            direction: older
            timestring: '%Y.%m.%d'
            unit: days
            unit_count: 7
            field:
            stats_result:
            epoch:
            exclude: False
    # Having config_yaml WILL override the other config
    config_yml: |-
      ---
      client:
        hosts:
          - elasticsearch-client
        port: 9200
        # url_prefix:
        # use_ssl: True
        # certificate:
        # client_cert:
        # client_key:
        # ssl_no_validate: True
        # http_auth:
        # timeout: 30
        # master_only: False
      # logging:
      #   loglevel: INFO
      #   logfile:
      #   logformat: default
      #   blacklist: ['elasticsearch', 'urllib3']

  resources:
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi

  priorityClassName: ""

  # extraVolumes and extraVolumeMounts allows you to mount other volumes
  # Example Use Case: mount ssl certificates when elasticsearch has tls enabled
  # extraVolumes:
  #   - name: es-certs
  #     secret:
  #       defaultMode: 420
  #       secretName: es-certs
  # extraVolumeMounts:
  #   - name: es-certs
  #     mountPath: /certs
  #     readOnly: true

  # Add your own init container or uncomment and modify the given example.
  extraInitContainers: {}
    ## Don't configure S3 repository till Elasticsearch is reachable.
    ## Ensure that it is available at http://elasticsearch:9200
    ##
    # elasticsearch-s3-repository:
    #   image: jwilder/dockerize:latest
    #   imagePullPolicy: "IfNotPresent"
    #   command:
    #   - "/bin/sh"
    #   - "-c"
    #   args:
    #   - |
    #     ES_HOST=elasticsearch
    #     ES_PORT=9200
    #     ES_REPOSITORY=backup
    #     S3_REGION=us-east-1
    #     S3_BUCKET=bucket
    #     S3_BASE_PATH=backup
    #     S3_COMPRESS=true
    #     S3_STORAGE_CLASS=standard
    #     apk add curl --no-cache && \
    #     dockerize -wait http://${ES_HOST}:${ES_PORT} --timeout 120s && \
    #     cat <<EOF | curl -sS -XPUT -H "Content-Type: application/json" -d @- http://${ES_HOST}:${ES_PORT}/_snapshot/${ES_REPOSITORY} \
    #     {
    #       "type": "s3",
    #       "settings": {
    #         "bucket": "${S3_BUCKET}",
    #         "base_path": "${S3_BASE_PATH}",
    #         "region": "${S3_REGION}",
    #         "compress": "${S3_COMPRESS}",
    #         "storage_class": "${S3_STORAGE_CLASS}"
  #       }
  #     }

  securityContext:
    runAsUser: 16  # run as cron user instead of root
